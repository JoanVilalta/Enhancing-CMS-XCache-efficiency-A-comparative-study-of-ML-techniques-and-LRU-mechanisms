{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e9bc99-66fc-4973-8e27-70e27ce5b39e",
   "metadata": {},
   "source": [
    "# XGBoost Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14b7944-414c-4a5d-a920-835c852fc83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRaw training script (no class balancing enabled).\\n\\nTo try a different class balancing method:\\n1. Comment out the “No class balancing” block below.\\n2. Uncomment the block for the method you want to use.\\n\\nFor the “scaled” method:\\n- Run the script as-is (no class balancing).\\n- Then uncomment the three `scale_pos_weight` parameters in the XGBoost model setup.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Raw training script (no class balancing enabled).\n",
    "\n",
    "To try a different class balancing method:\n",
    "1. Comment out the “No class balancing” block below.\n",
    "2. Uncomment the block for the method you want to use.\n",
    "\n",
    "For the “scaled” method:\n",
    "- Run the script as-is (no class balancing).\n",
    "- Then uncomment the three `scale_pos_weight` parameters in the XGBoost model setup.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81905c53-ae5d-40b8-8e05-9a38300ec4a1",
   "metadata": {},
   "source": [
    "## General libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2898e10-919d-441c-bb3f-2f1599870cb5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from scipy.stats import uniform, randint\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='xgboost')\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import EarlyStopping\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc, average_precision_score, precision_recall_curve, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35211816-f0e2-48d7-bc93-4a8681fc19cd",
   "metadata": {},
   "source": [
    "## Sources of Data and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e000baaa-1e5c-4464-be23-6ae56fa372e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_pickle(\"./df_final_xCache.pkl\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "d_label = df_orig['d_label']\n",
    "d_label_encoded = le.fit_transform(d_label)\n",
    "df_orig['d_label_encoded'] = d_label_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86401854-83b7-4d92-8bfe-d335ce6de494",
   "metadata": {},
   "source": [
    "## Generation of periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69404722-cdcb-42bb-ab72-e61de0264d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-04-30'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0344def2-40a8-4de0-965b-50ea112619e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 45 realizations \n",
      "\n",
      "Train: [['2023-06-01', '2023-06-15'], ['2023-06-08', '2023-06-22'], ['2023-06-15', '2023-06-29'], ['2023-06-22', '2023-07-06'], ['2023-06-29', '2023-07-13'], ['2023-07-06', '2023-07-20'], ['2023-07-13', '2023-07-27'], ['2023-07-20', '2023-08-03'], ['2023-07-27', '2023-08-10'], ['2023-08-03', '2023-08-17'], ['2023-08-10', '2023-08-24'], ['2023-08-17', '2023-08-31'], ['2023-08-24', '2023-09-07'], ['2023-08-31', '2023-09-14'], ['2023-09-07', '2023-09-21'], ['2023-09-14', '2023-09-28'], ['2023-09-21', '2023-10-05'], ['2023-09-28', '2023-10-12'], ['2023-10-05', '2023-10-19'], ['2023-10-12', '2023-10-26'], ['2023-10-19', '2023-11-02'], ['2023-10-26', '2023-11-09'], ['2023-11-02', '2023-11-16'], ['2023-11-09', '2023-11-23'], ['2023-11-16', '2023-11-30'], ['2023-11-23', '2023-12-07'], ['2023-11-30', '2023-12-14'], ['2023-12-07', '2023-12-21'], ['2023-12-14', '2023-12-28'], ['2023-12-21', '2024-01-04'], ['2023-12-28', '2024-01-11'], ['2024-01-04', '2024-01-18'], ['2024-01-11', '2024-01-25'], ['2024-01-18', '2024-02-01'], ['2024-01-25', '2024-02-08'], ['2024-02-01', '2024-02-15'], ['2024-02-08', '2024-02-22'], ['2024-02-15', '2024-02-29'], ['2024-02-22', '2024-03-07'], ['2024-02-29', '2024-03-14'], ['2024-03-07', '2024-03-21'], ['2024-03-14', '2024-03-28'], ['2024-03-21', '2024-04-04'], ['2024-03-28', '2024-04-11'], ['2024-04-04', '2024-04-18']] \n",
      "\n",
      "Future Accesses: [['2023-06-15', '2023-06-22'], ['2023-06-22', '2023-06-29'], ['2023-06-29', '2023-07-06'], ['2023-07-06', '2023-07-13'], ['2023-07-13', '2023-07-20'], ['2023-07-20', '2023-07-27'], ['2023-07-27', '2023-08-03'], ['2023-08-03', '2023-08-10'], ['2023-08-10', '2023-08-17'], ['2023-08-17', '2023-08-24'], ['2023-08-24', '2023-08-31'], ['2023-08-31', '2023-09-07'], ['2023-09-07', '2023-09-14'], ['2023-09-14', '2023-09-21'], ['2023-09-21', '2023-09-28'], ['2023-09-28', '2023-10-05'], ['2023-10-05', '2023-10-12'], ['2023-10-12', '2023-10-19'], ['2023-10-19', '2023-10-26'], ['2023-10-26', '2023-11-02'], ['2023-11-02', '2023-11-09'], ['2023-11-09', '2023-11-16'], ['2023-11-16', '2023-11-23'], ['2023-11-23', '2023-11-30'], ['2023-11-30', '2023-12-07'], ['2023-12-07', '2023-12-14'], ['2023-12-14', '2023-12-21'], ['2023-12-21', '2023-12-28'], ['2023-12-28', '2024-01-04'], ['2024-01-04', '2024-01-11'], ['2024-01-11', '2024-01-18'], ['2024-01-18', '2024-01-25'], ['2024-01-25', '2024-02-01'], ['2024-02-01', '2024-02-08'], ['2024-02-08', '2024-02-15'], ['2024-02-15', '2024-02-22'], ['2024-02-22', '2024-02-29'], ['2024-02-29', '2024-03-07'], ['2024-03-07', '2024-03-14'], ['2024-03-14', '2024-03-21'], ['2024-03-21', '2024-03-28'], ['2024-03-28', '2024-04-04'], ['2024-04-04', '2024-04-11'], ['2024-04-11', '2024-04-18'], ['2024-04-18', '2024-04-25']]\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime(2023, 6, 1)\n",
    "\n",
    "# Function to generate periods\n",
    "def generate_periods(start_date, periods, days_per_period):\n",
    "    result = []\n",
    "    \n",
    "    # Loop through the number of periods and create 15-day intervals\n",
    "    for _ in range(periods):\n",
    "        end_date = start_date + timedelta(days=days_per_period)\n",
    "        # Convert to string with the format \"%Y-%m-%d\"\n",
    "        period_str = (start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))\n",
    "        result.append(period_str)\n",
    "        start_date = end_date  # Move the start date to the end date of the current period\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "periods = generate_periods(start_date, 47, 7)\n",
    "\n",
    "p = periods[0][0]\n",
    "m = periods[1][1]\n",
    "\n",
    "train_p = []\n",
    "fut_acc_p = []\n",
    "\n",
    "for i in range(0,len(periods)-2):\n",
    "    train_p.append([periods[i][0], periods[i+1][1]])\n",
    "    fut_acc_p.append([periods[i+1][1], periods[i+2][1]])\n",
    "\n",
    "print(\"We have\", len(train_p), \"realizations \\n\")\n",
    "print(\"Train:\", train_p, '\\n')\n",
    "print(\"Future Accesses:\", fut_acc_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6966c5-4429-436d-ae6f-a43d9a8ce649",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb0247-4cde-4669-acd6-ab578f2f181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain = False\n",
    "useFiles = False\n",
    "createModel = True\n",
    "\n",
    "rl_a = []\n",
    "\n",
    "accesses_train = []\n",
    "accesses_fut_acc = []\n",
    "\n",
    "unique_files_train = []\n",
    "unique_files_fut_acc = []\n",
    "\n",
    "roc_curves = []\n",
    "pr_curves = []\n",
    "\n",
    "importances_by_period = {}\n",
    "\n",
    "\n",
    "# directory to save XGBoost decision trees\n",
    "path1 = f\"./XGBoost/raw/trees/Decision_Tree_XGB\"\n",
    "\n",
    "# directory to save XGBoost results\n",
    "path2 = f\"./XGBoost/raw/results/Results_XGB\"\n",
    "\n",
    "\n",
    "gif_dir = os.path.join(path2, \"gif\")\n",
    "results_txt = os.path.join(path2, \"Results.txt\")\n",
    "\n",
    "os.makedirs(path1, exist_ok=True)\n",
    "os.makedirs(gif_dir, exist_ok=True)\n",
    "\n",
    "if os.path.exists(results_txt):\n",
    "    os.remove(results_txt)\n",
    "    \n",
    "all_preds_by_o = []\n",
    "all_labels_by_o = []\n",
    "\n",
    "results = {\n",
    "    \"auc\": [[], []],\n",
    "    \"acc\": [[], []],\n",
    "    \"rec\": [[], []],\n",
    "    \"prec\": [[], []],\n",
    "    \"f1\": [[], []]\n",
    "}\n",
    "\n",
    "f = open(f\"{path2}/Results.txt\", \"a\")\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for rl in range(0,len(train_p)):\n",
    "    ii+=1\n",
    "    print(\"############ \", ii)\n",
    "    rl_a.append(str(rl+1))\n",
    "\n",
    "    # Sample generation\n",
    "\n",
    "    df1 = df_orig[ (df_orig['date'] >= train_p[rl][0]) & (df_orig['date'] < train_p[rl][1])].copy()\n",
    "    df2 = df_orig[ (df_orig['date'] >= fut_acc_p[rl][0]) & (df_orig['date'] < fut_acc_p[rl][1])].copy()\n",
    "\n",
    "    accesses_train.append(df1.shape[0])\n",
    "    accesses_fut_acc.append(df2.shape[0])\n",
    "    unique_files_train.append(len(df1['filename'].unique()))\n",
    "    unique_files_fut_acc.append(len(df2['filename'].unique()))\n",
    "   \n",
    "    # Add new column for the Accesses in the train    \n",
    "    df1['Access'] = df1.groupby(['filename']).cumcount().add(1)\n",
    "    df1['Total Accesses'] = df1.groupby('filename').filename.transform('count')\n",
    "    \n",
    "    # Add new column with the read access to period\n",
    "    date_string = df2['date'].max() + \" 23:59:59.999999\"\n",
    "    dt_object = datetime.strptime(date_string, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    epoch_time_period1 = dt_object.timestamp()\n",
    "    df1['read_access time'] = (epoch_time_period1 - df1['time'])\n",
    "\n",
    "    # Add new column for the Future Access in the train and test periods       \n",
    "    df1['future_acc'] = df1.filename.isin(df2.filename).astype(int)  \n",
    "    df1.sort_values(['filename','time'], ascending=[True,True], inplace = True)\n",
    "            \n",
    "    # Calculate the last 5 accesses\n",
    "    df1[\"5th last read access\"] = df1['read_access time'].shift(4)\n",
    "    df1[\"4th last read access\"] = df1['read_access time'].shift(3)\n",
    "    df1[\"3rd last read access\"] = df1['read_access time'].shift(2)\n",
    "    df1[\"2nd last read access\"] = df1['read_access time'].shift(1)\n",
    "    df1[\"last read access\"] = df1['read_access time']\n",
    "\n",
    "    df1.loc[df1['Total Accesses'] == 1, '2nd last read access'] = np.nan\n",
    "    df1.loc[df1['Total Accesses'] == 1, '3rd last read access'] = np.nan\n",
    "    df1.loc[df1['Total Accesses'] == 1, '4th last read access'] = np.nan\n",
    "    df1.loc[df1['Total Accesses'] == 1, '5th last read access'] = np.nan\n",
    "        \n",
    "    df1.loc[df1['Total Accesses'] == 2, '3rd last read access'] = np.nan\n",
    "    df1.loc[df1['Total Accesses'] == 2, '4th last read access'] = np.nan\n",
    "    df1.loc[df1['Total Accesses'] == 2, '5th last read access'] = np.nan\n",
    "        \n",
    "    df1.loc[df1['Total Accesses'] == 3, '4th last read access'] = np.nan\n",
    "    df1.loc[df1['Total Accesses'] == 3, '5th last read access'] = np.nan\n",
    "        \n",
    "    df1.loc[df1['Total Accesses'] == 4, '5th last read access'] = np.nan\n",
    "        \n",
    "    # Calculate first accesses\n",
    "    df1.sort_values(['filename','time'], ascending=[True,True], inplace = True)\n",
    "\n",
    "    df1_first=df1.drop_duplicates(subset = ['filename'], keep = 'first', inplace = False)\n",
    "    df1_last=df1.drop_duplicates(subset = ['filename'], keep = 'last', inplace = False)\n",
    "    df1_first_list=list(df1_first[\"time\"])\n",
    "    df1_last_list=list(df1_last[\"time\"])\n",
    "        \n",
    "    # DeltaT 1st access and most recent:\n",
    "    dt_1st_to_last = [(x1 - x2) for (x1, x2) in zip(df1_last_list, df1_first_list)]\n",
    "    dt_1st_to_last = [0 if x==0 else x for x in dt_1st_to_last]\n",
    "        \n",
    "    # Recency 1st access\n",
    "    recency_1st = [(epoch_time_period1 - x) for x in df1_first_list]\n",
    "        \n",
    "    df1.sort_values(['filename','time'], ascending=[True,True], inplace = True)\n",
    "    df1_final = df1.copy()\n",
    "    df1_final.drop_duplicates(subset = ['filename'], keep = 'last', inplace = True)\n",
    "    df1_final.sort_values(by='filename', ascending=True, inplace = True)\n",
    "        \n",
    "    df1_final.loc[:, 'deltaT_1_last'] = dt_1st_to_last\n",
    "    df1_final.loc[:, 'recency_1st'] = recency_1st\n",
    "    \n",
    "    df1_final['future_acc'] = df1_final.pop('future_acc')\n",
    "\n",
    "    \n",
    "    if ii <=1:\n",
    "        df1_final_cp = df1_final.copy()\n",
    "    else:\n",
    "        df1_final_cp = pd.concat([df1_final.copy(), df1_final_cp])\n",
    "\n",
    "    \n",
    "    threshold_time = pd.to_datetime(df1_final_cp['date'].max()) - pd.DateOffset(months=3)\n",
    "    df1_final_cp = df1_final_cp[df1_final_cp['date'] >= threshold_time.strftime(\"%Y-%m-%d\")]\n",
    "    df1_final_cp = df1_final_cp[df1_final_cp['date'] >= threshold_time.strftime(\"%Y-%m-%d\")]\n",
    "    \n",
    "\n",
    "    df1_final_c = df1_final_cp.copy()\n",
    "    \n",
    "    f_test=0.3\n",
    "    \n",
    "    df1_final_c.sort_values(by='time', ascending=True, inplace=True)\n",
    "    \n",
    "\n",
    "    ##########################\n",
    "    # Undersampling\n",
    "    # df1_final_var = df1_final_c.drop(columns=['future_acc']).copy()\n",
    "    # df1_final_label = df1_final_c['future_acc']\n",
    "\n",
    "    # under = RandomUnderSampler(sampling_strategy=1)\n",
    "    # df1_final_var_under, df1_final_label_under = under.fit_resample(df1_final_var, df1_final_label)\n",
    "\n",
    "    # df1_final_under = pd.concat([df1_final_var_under, df1_final_label_under], axis=1)\n",
    "\n",
    "    # string_train_date = f\"Train: ['{df1_final_under['date'].min()}', '{df1_final_under['date'].max()}']\"\n",
    "    # string_fut_date = f\"Future Accesses: {str(fut_acc_p[rl])}\\n\"\n",
    "\n",
    "    # print(f\"Train: ['{df1_final_under['date'].min()}', '{df1_final_under['date'].max()}']\")\n",
    "    # print(f\"Future Accesses: {str(fut_acc_p[rl])}\\n\")\n",
    "\n",
    "    # df1_final = df1_final_under.copy()\n",
    "\n",
    "    \n",
    "    ##########################\n",
    "    # Tomek links\n",
    "\n",
    "    # df1_final_label = df1_final_c['future_acc']\n",
    "\n",
    "    # df1_final_var = df1_final_c.select_dtypes(include=['number'])\n",
    "    # df1_final_var = df1_final_var.drop(columns=['future_acc'])\n",
    "    \n",
    "    # df1_final_var = df1_final_var.fillna(-1.0)\n",
    "    \n",
    "    # tl = TomekLinks(sampling_strategy='auto')\n",
    "    # df1_final_var_under, df1_final_label_under = tl.fit_resample(df1_final_var, df1_final_label)\n",
    "\n",
    "    # df1_final_under = pd.concat([df1_final_var_under, df1_final_label_under], axis=1)\n",
    "\n",
    "    # df1_final_under = df1_final_under.replace(-1.0, np.nan)\n",
    "\n",
    "    # df1_final = df1_final_under.copy()\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    # Edited Nearest Neighbours\n",
    "\n",
    "    # df1_final_label = df1_final_c['future_acc']\n",
    "\n",
    "    # df1_final_var = df1_final_c.select_dtypes(include=['number'])\n",
    "    # df1_final_var = df1_final_var.drop(columns=['future_acc'])\n",
    "    \n",
    "    # df1_final_var = df1_final_var.fillna(-1.0)\n",
    "    \n",
    "    # enn = EditedNearestNeighbours(sampling_strategy='auto')\n",
    "    # df1_final_var_under, df1_final_label_under = enn.fit_resample(df1_final_var, df1_final_label)\n",
    "\n",
    "    # df1_final_under = pd.concat([df1_final_var_under, df1_final_label_under], axis=1)\n",
    "\n",
    "    # df1_final_under = df1_final_under.replace(-1.0, np.nan)\n",
    "\n",
    "    # df1_final = df1_final_under.copy()\n",
    "\n",
    "    ##########################\n",
    "    #SMOTE\n",
    "    # string_train_date = f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\"\n",
    "    # string_fut_date = f\"Future Accesses: {str(fut_acc_p[rl])}\\n\"\n",
    "\n",
    "    # print(f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\")\n",
    "    # print(f\"Future Accesses: {str(fut_acc_p[rl])}\\n\")\n",
    "\n",
    "    # df1_final_c = df1_final_c.sample(frac = 1).copy()\n",
    "\n",
    "    # df1_final_c = df1_final_c.fillna(-1.0)\n",
    "    \n",
    "    # df1_final_var = df1_final_c.drop(columns=['future_acc']).copy()\n",
    "    # df1_final_label = df1_final_c['future_acc']\n",
    "    \n",
    "    # df1_final_var_numeric = df1_final_var.select_dtypes(include=['number'])\n",
    "\n",
    "    # smote = SMOTE(sampling_strategy=1)\n",
    "    # df1_final_var_resampled, df1_final_label_resampled = smote.fit_resample(df1_final_var_numeric, df1_final_label)\n",
    "\n",
    "    # df1_final_resampled = pd.concat([df1_final_var_resampled, df1_final_label_resampled], axis=1)\n",
    "\n",
    "    # df1_final_resampled['future_acc'] = df1_final_resampled['future_acc'].astype(int)\n",
    "\n",
    "    # df1_final_resampled = df1_final_resampled.replace(-1.0, np.nan)\n",
    "    \n",
    "    # df1_final = df1_final_resampled.copy()\n",
    "\n",
    "    ##########################\n",
    "    #SMOTEEN\n",
    "    # string_train_date = f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\"\n",
    "    # string_fut_date = f\"Future Accesses: {str(fut_acc_p[rl])}\\n\"\n",
    "\n",
    "    # print(f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\")\n",
    "    # print(f\"Future Accesses: {str(fut_acc_p[rl])}\\n\")\n",
    "\n",
    "    # df1_final = df1_final_c.sample(frac = 1).copy()\n",
    "\n",
    "    # df1_final_c = df1_final_c.fillna(-1.0)\n",
    "    \n",
    "    # df1_final_var = df1_final_c.drop(columns=['future_acc']).copy()\n",
    "    # df1_final_label = df1_final_c['future_acc']\n",
    "    \n",
    "    # df1_final_var_numeric = df1_final_var.select_dtypes(include=['number'])\n",
    "\n",
    "    # smoteenn = SMOTEENN(sampling_strategy=1)\n",
    "    # df1_final_var_resampled, df1_final_label_resampled = smoteenn.fit_resample(df1_final_var_numeric, df1_final_label)\n",
    "\n",
    "    # df1_final_resampled = pd.concat([df1_final_var_resampled, df1_final_label_resampled], axis=1)\n",
    "\n",
    "    # df1_final_resampled['future_acc'] = df1_final_resampled['future_acc'].astype(int)\n",
    "\n",
    "    # df1_final_resampled = df1_final_resampled.replace(-1.0, np.nan)\n",
    "\n",
    "    # df1_final = df1_final_resampled.copy()\n",
    "\n",
    "    ##########################\n",
    "    #SMOTE TOMEK\n",
    "    # string_train_date = f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\"\n",
    "    # string_fut_date = f\"Future Accesses: {str(fut_acc_p[rl])}\\n\"\n",
    "\n",
    "    # print(f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\")\n",
    "    # print(f\"Future Accesses: {str(fut_acc_p[rl])}\\n\")\n",
    "\n",
    "    # df1_final = df1_final_c.sample(frac = 1).copy()\n",
    "\n",
    "    # df1_final_c = df1_final_c.fillna(-1.0)\n",
    "    \n",
    "    # df1_final_var = df1_final_c.drop(columns=['future_acc']).copy()\n",
    "    # df1_final_label = df1_final_c['future_acc']\n",
    "    \n",
    "    # df1_final_var_numeric = df1_final_var.select_dtypes(include=['number'])\n",
    "\n",
    "    # smtk = SMOTETomek(sampling_strategy=1)\n",
    "    # df1_final_var_resampled, df1_final_label_resampled = smtk.fit_resample(df1_final_var_numeric, df1_final_label)\n",
    "\n",
    "    # df1_final_resampled = pd.concat([df1_final_var_resampled, df1_final_label_resampled], axis=1)\n",
    "\n",
    "    # df1_final_resampled['future_acc'] = df1_final_resampled['future_acc'].astype(int)\n",
    "\n",
    "    # df1_final_resampled = df1_final_resampled.replace(-1.0, np.nan)\n",
    "\n",
    "    # df1_final = df1_final_resampled.copy()\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    # No class balance\n",
    "\n",
    "    string_train_date = f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\"\n",
    "    string_fut_date = f\"Future Accesses: {str(fut_acc_p[rl])}\\n\"\n",
    "\n",
    "    print(f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\")\n",
    "    print(f\"Future Accesses: {str(fut_acc_p[rl])}\\n\")\n",
    "\n",
    "    df1_final = df1_final_c.sample(frac = 1).copy()\n",
    "\n",
    "    \n",
    "    ##########################\n",
    "    # End of class balance methods\n",
    "\n",
    "\n",
    "\n",
    "    # -----Continuation-----\n",
    "    # Apply a logarithmic transformation to the required columns\n",
    "\n",
    "    log_vars =  ['Total Accesses', '5th last read access', '4th last read access',\n",
    "                 '3rd last read access', '2nd last read access','last read access',\n",
    "                'recency_1st', 'deltaT_1_last']\n",
    "\n",
    "    for var in log_vars:\n",
    "        df1_final[var] = np.log1p(df1_final[var])\n",
    "        \n",
    "    \n",
    "\n",
    "    thevars = ['last read access','recency_1st', 'deltaT_1_last', '2nd last read access',\n",
    "               '3rd last read access', '4th last read access','5th last read access', \n",
    "               'Total Accesses', 'd_label_encoded', 'future_acc']\n",
    "\n",
    "    \n",
    "    df1_final['future_acc'] = df1_final['future_acc'].astype(int)\n",
    "    \n",
    "    df1_final = df1_final[thevars]\n",
    "\n",
    "\n",
    "    # Nested Cross Validation\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    all_best_params = []\n",
    "\n",
    "    auc_scores, acc_scores, rec_scores, prec_scores, f1_scores = [], [], [], [], []\n",
    "    \n",
    "    for outer_fold, (train_val_idx, test_idx) in enumerate(outer_cv.split(df1_final.drop(columns=['future_acc']), df1_final['future_acc'])):\n",
    "    \n",
    "        df1_train_val = df1_final.iloc[train_val_idx]\n",
    "        df1_test = df1_final.iloc[test_idx]\n",
    "\n",
    "        train_valL = df1_train_val['future_acc'].values\n",
    "        testL = df1_test['future_acc'].values\n",
    "    \n",
    "        train_valD = df1_train_val.drop(columns=['future_acc']).values\n",
    "        testD = df1_test.drop(columns=['future_acc']).values\n",
    "    \n",
    "\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=f_test)\n",
    "        for train_index, val_index in sss.split(train_valD, train_valL):\n",
    "            trainD, valD = train_valD[train_index], train_valD[val_index]\n",
    "            trainL, valL = train_valL[train_index], train_valL[val_index]\n",
    "    \n",
    "        # Calculate scale_pos_weight to handle class imbalance within the fold\n",
    "        scale = len(trainL[trainL == 0]) / len(trainL[trainL == 1])\n",
    "\n",
    "        \n",
    "        param_dist = {\n",
    "            'n_estimators': randint(100, 400),\n",
    "            'max_depth': randint(3, 10),\n",
    "            'learning_rate': uniform(0.01, 0.3),\n",
    "            'subsample': uniform(0.7, 0.3),\n",
    "            'colsample_bytree': uniform(0.7, 0.3),\n",
    "            'reg_lambda': uniform(0, 2),\n",
    "            'reg_alpha': uniform(0, 1)\n",
    "        }\n",
    "    \n",
    "        model = XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            tree_method='gpu_hist',\n",
    "            predictor='gpu_predictor',\n",
    "            # scale_pos_weight=scale\n",
    "        )\n",
    "\n",
    "        inner_cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    \n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=20,\n",
    "            cv=inner_cv,\n",
    "            n_jobs=1,\n",
    "            scoring='roc_auc',\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "        random_search.fit(trainD, trainL)\n",
    "    \n",
    "        best_params = random_search.best_params_\n",
    "        all_best_params.append(best_params)\n",
    "    \n",
    "        dtrain = xgb.DMatrix(trainD, label=trainL)\n",
    "        dval = xgb.DMatrix(valD, label=valL)\n",
    "    \n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss',\n",
    "            'tree_method': 'hist',\n",
    "            'device': 'cuda',\n",
    "            # 'scale_pos_weight': scale,\n",
    "            **best_params\n",
    "        }\n",
    "    \n",
    "        model_fold = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dval, 'val')],\n",
    "            early_stopping_rounds=10,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "    \n",
    "        dtest = xgb.DMatrix(testD)\n",
    "        pred_proba = model_fold.predict(dtest)\n",
    "\n",
    "        pred_labels = (pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        auc_score = roc_auc_score(testL, pred_proba)\n",
    "        acc = accuracy_score(testL, pred_labels)\n",
    "        rec = recall_score(testL, pred_labels)\n",
    "        prec = precision_score(testL, pred_labels)\n",
    "        f1 = f1_score(testL, pred_labels)\n",
    "    \n",
    "        # print(f\"AUC Fold {outer_fold+1}: {auc_score:.4f}\")\n",
    "        \n",
    "        auc_scores.append(auc_score)\n",
    "        acc_scores.append(acc)\n",
    "        rec_scores.append(rec)\n",
    "        prec_scores.append(prec)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "    \n",
    "    results[\"auc\"][0].append(np.mean(auc_scores))\n",
    "    results[\"auc\"][1].append(np.std(auc_scores, ddof = 1)/np.sqrt(len(auc_scores)))\n",
    "    \n",
    "    results[\"acc\"][0].append(np.mean(acc_scores))\n",
    "    results[\"acc\"][1].append(np.std(acc_scores, ddof = 1)/np.sqrt(len(acc_scores)))\n",
    "    \n",
    "    results[\"rec\"][0].append(np.mean(rec_scores))\n",
    "    results[\"rec\"][1].append(np.std(rec_scores, ddof = 1)/np.sqrt(len(rec_scores)))\n",
    "    \n",
    "    results[\"prec\"][0].append(np.mean(prec_scores))\n",
    "    results[\"prec\"][1].append(np.std(prec_scores, ddof = 1)/np.sqrt(len(prec_scores)))\n",
    "    \n",
    "    results[\"f1\"][0].append(np.mean(f1_scores))\n",
    "    results[\"f1\"][1].append(np.std(f1_scores, ddof = 1)/np.sqrt(len(f1_scores)))\n",
    "    \n",
    "    # print(f\"Mean AUC (Nested CV): {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "\n",
    "    param_sums = defaultdict(float)\n",
    "    \n",
    "    for bp in all_best_params:\n",
    "        for key, value in bp.items():\n",
    "            param_sums[key] += value\n",
    "    \n",
    "    int_params = ['n_estimators', 'max_depth']\n",
    "\n",
    "    best_params_avg = {}\n",
    "    for key in param_sums:\n",
    "        mean_value = param_sums[key] / len(all_best_params)\n",
    "        if key in int_params:\n",
    "            best_params_avg[key] = int(round(mean_value))\n",
    "        else:\n",
    "            best_params_avg[key] = float(mean_value)\n",
    "    \n",
    "    # print(\"Best_params average:\", best_params_avg)\n",
    "\n",
    "    dtrain_final = xgb.DMatrix(df1_final.drop(columns=['future_acc']).values, label=df1_final['future_acc'].values)\n",
    "    \n",
    "    scale = len(trainL[trainL == 0]) / len(trainL[trainL == 1])\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda',\n",
    "        # 'scale_pos_weight': scale,\n",
    "        **best_params_avg\n",
    "    }\n",
    "    \n",
    "\n",
    "    model_final = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain_final,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain_final, 'train')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    model_final.save_model(f'{path1}/model_xgb_{int(epoch_time_period1)}.ubj')\n",
    "    \n",
    "    dtest = xgb.DMatrix(testD)\n",
    "    pred_proba = model_final.predict(dtest)  \n",
    "    pred_labels = (pred_proba > 0.5).astype(int)\n",
    "\n",
    "    \n",
    "    ###############\n",
    "    #Predicted probability distribution by period\n",
    "    pred_probs_reaccessed = pred_proba[testL  == 1]  \n",
    "    pred_probs_non_reaccessed = pred_proba[testL  == 0] \n",
    "    \n",
    "    plt.hist(pred_probs_reaccessed, bins=50, color='blue', alpha=0.7, label='Reaccessed', histtype='step', linewidth=2)\n",
    "    plt.hist(pred_probs_non_reaccessed, bins=50, color='orange', alpha=0.7, label='Not Reaccessed', histtype='step', linewidth=2)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Probabilities')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.ylim(1, 100000)\n",
    "    plt.text(0.88, 2300, ii , fontsize=14, ha='left', va='bottom')\n",
    "    plt.savefig(f\"{path2}/gif/hist_gif{ii}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    all_preds_by_o.append(pred_proba)\n",
    "    all_labels_by_o.append(testL)\n",
    "\n",
    "    accuracy = accuracy_score(testL, pred_labels)\n",
    "    recall = recall_score(testL, pred_labels)\n",
    "    precision = precision_score(testL, pred_labels)\n",
    "    f1 = f1_score(testL, pred_labels)\n",
    "    auc_a = roc_auc_score(testL, pred_proba)\n",
    "\n",
    "    print(f\"\\n--- Results ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc_a:.4f}\\n\")\n",
    "\n",
    "    importances_by_period[o] = []\n",
    "\n",
    "    feature_names = df1_final.drop(columns=['future_acc']).columns.tolist()\n",
    "\n",
    "    importance_dict = model_final.get_score(importance_type='weight')\n",
    "\n",
    "    mapped_importance = {\n",
    "        feature_names[int(k[1:])]: v for k, v in importance_dict.items()\n",
    "        if k.startswith('f') and k[1:].isdigit() and int(k[1:]) < len(feature_names)\n",
    "    }\n",
    "\n",
    "    importances_by_period[o].append(mapped_importance)\n",
    "\n",
    "    \n",
    "    \n",
    "###############\n",
    "#Output results\n",
    "str_to_file = (\n",
    "    f\"{','.join(map(str, results['auc'][0]))}\\n\"\n",
    "    f\"{','.join(map(str, results['auc'][1]))}\\n\"\n",
    "    f\"{','.join(map(str, results['acc'][0]))}\\n\"\n",
    "    f\"{','.join(map(str, results['acc'][1]))}\\n\"\n",
    "    f\"{','.join(map(str, results['rec'][0]))}\\n\"\n",
    "    f\"{','.join(map(str, results['rec'][1]))}\\n\"\n",
    "    f\"{','.join(map(str, results['prec'][0]))}\\n\"\n",
    "    f\"{','.join(map(str, results['prec'][1]))}\\n\"\n",
    "    f\"{','.join(map(str, results['f1'][0]))}\\n\"\n",
    "    f\"{','.join(map(str, results['f1'][1]))}\\n\"\n",
    ")\n",
    "\n",
    "with open(f\"{path2}/Results.txt\", \"a\") as f:\n",
    "    f.write(str_to_file)\n",
    "        \n",
    "\n",
    "all_preds_flat = np.concatenate(all_preds_by_o)\n",
    "all_labels_flat = np.concatenate(all_labels_by_o)\n",
    "\n",
    "overall_auc = roc_auc_score(all_labels_flat, all_preds_flat)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(all_labels_flat, all_preds_flat)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "roc_curves.append((fpr, tpr, roc_auc, o))\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(all_labels_flat, all_preds_flat)\n",
    "ap = average_precision_score(all_labels_flat, all_preds_flat)\n",
    "pr_curves.append((recall, precision, ap, o))\n",
    "\n",
    "\n",
    "###############\n",
    "#Total redicted probability distribution\n",
    "pred_probs_reaccessed = all_preds_flat[all_labels_flat  == 1]  \n",
    "pred_probs_non_reaccessed = all_preds_flat[all_labels_flat  == 0] \n",
    "\n",
    "plt.hist(pred_probs_reaccessed, bins=50, color='blue', alpha=0.7, label='Reaccessed', histtype='step', linewidth=2)\n",
    "plt.hist(pred_probs_non_reaccessed, bins=50, color='orange', alpha=0.7, label='Not reaccessed', histtype='step', linewidth=2)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Probabilities')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path2}/hist_probabilities.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "###############\n",
    "#Confusion matrix\n",
    "all_pred_labels = (all_preds_flat > 0.5).astype(int)\n",
    "\n",
    "class_names = ['Not reaccessed', 'Reaccessed']\n",
    "\n",
    "cm = confusion_matrix(all_labels_flat, all_pred_labels)\n",
    "\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=0, keepdims=True) * 100\n",
    "\n",
    "plt.imshow(cm_percent, interpolation='nearest', cmap='Blues')\n",
    "plt.colorbar(label='% of prediction')\n",
    "\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        pct = cm_percent[i, j]\n",
    "        plt.text(j, i, f\"{labels[i, j]} = {pct:.1f}%\", \n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if pct > cm_percent.max()/2 else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Prediction label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path2}/confusion_matrix.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###############\n",
    "#Feature Importance\n",
    "average_importance = defaultdict(float)\n",
    "count_importance = defaultdict(int)\n",
    "\n",
    "for imp in importances_by_period[o]:\n",
    "    for feat, val in imp.items():\n",
    "        average_importance[feat] += val\n",
    "        count_importance[feat] += 1\n",
    "\n",
    "for feat in average_importance:\n",
    "    average_importance[feat] /= count_importance[feat]\n",
    "\n",
    "importance_df = pd.DataFrame(\n",
    "    list(average_importance.items()), columns=['Feature', 'Importance']\n",
    ").sort_values(by='Importance', ascending=True)\n",
    "\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Average Importance')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path2}/feature_importance.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###############\n",
    "#Correlation matrix\n",
    "feature_columns = df1_final.drop(columns=['future_acc']).columns\n",
    "\n",
    "corr_matrix = df1_final[feature_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='viridis', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path2}/correlation_matrix.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###############\n",
    "#Precision-Recall\n",
    "for recall, precision, ap, o in pr_curves:\n",
    "    plt.plot(recall, precision, lw=1.5, label=f'AP = {ap:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid()\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path2}/corves_precision_recall.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "with open(f\"{path2}/pr_curves_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pr_curves, f)\n",
    "\n",
    "###############\n",
    "#ROC Curves\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey', lw=1)\n",
    "for fpr, tpr, roc_auc, o in roc_curves:\n",
    "    plt.plot(fpr, tpr, lw=1.5, label=f'AUC = {roc_auc:.4f}')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path2}/corves_roc.png\", dpi=300)   \n",
    "plt.show()\n",
    "\n",
    "with open(f\"{path2}/roc_curves_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(roc_curves, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea9c815-0077-43e5-b69d-07ae48f6a31d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jvilalta",
   "language": "python",
   "name": "jvilalta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
