{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e9bc99-66fc-4973-8e27-70e27ce5b39e",
   "metadata": {},
   "source": [
    "# XGBoost Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81905c53-ae5d-40b8-8e05-9a38300ec4a1",
   "metadata": {},
   "source": [
    "## General libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2898e10-919d-441c-bb3f-2f1599870cb5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from scipy.stats import uniform, randint\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='xgboost')\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import EarlyStopping\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc, average_precision_score, precision_recall_curve, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35211816-f0e2-48d7-bc93-4a8681fc19cd",
   "metadata": {},
   "source": [
    "## Sources of Data and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e000baaa-1e5c-4464-be23-6ae56fa372e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_pickle(\"./df_final_xCache.pkl\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "d_label = df_orig['d_label']\n",
    "d_label_encoded = le.fit_transform(d_label)\n",
    "df_orig['d_label_encoded'] = d_label_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86401854-83b7-4d92-8bfe-d335ce6de494",
   "metadata": {},
   "source": [
    "## Generation of periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69404722-cdcb-42bb-ab72-e61de0264d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-04-30'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0344def2-40a8-4de0-965b-50ea112619e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 46 realizations \n",
      "\n",
      "Train: [['2023-06-01', '2023-06-15'], ['2023-06-08', '2023-06-22'], ['2023-06-15', '2023-06-29'], ['2023-06-22', '2023-07-06'], ['2023-06-29', '2023-07-13'], ['2023-07-06', '2023-07-20'], ['2023-07-13', '2023-07-27'], ['2023-07-20', '2023-08-03'], ['2023-07-27', '2023-08-10'], ['2023-08-03', '2023-08-17'], ['2023-08-10', '2023-08-24'], ['2023-08-17', '2023-08-31'], ['2023-08-24', '2023-09-07'], ['2023-08-31', '2023-09-14'], ['2023-09-07', '2023-09-21'], ['2023-09-14', '2023-09-28'], ['2023-09-21', '2023-10-05'], ['2023-09-28', '2023-10-12'], ['2023-10-05', '2023-10-19'], ['2023-10-12', '2023-10-26'], ['2023-10-19', '2023-11-02'], ['2023-10-26', '2023-11-09'], ['2023-11-02', '2023-11-16'], ['2023-11-09', '2023-11-23'], ['2023-11-16', '2023-11-30'], ['2023-11-23', '2023-12-07'], ['2023-11-30', '2023-12-14'], ['2023-12-07', '2023-12-21'], ['2023-12-14', '2023-12-28'], ['2023-12-21', '2024-01-04'], ['2023-12-28', '2024-01-11'], ['2024-01-04', '2024-01-18'], ['2024-01-11', '2024-01-25'], ['2024-01-18', '2024-02-01'], ['2024-01-25', '2024-02-08'], ['2024-02-01', '2024-02-15'], ['2024-02-08', '2024-02-22'], ['2024-02-15', '2024-02-29'], ['2024-02-22', '2024-03-07'], ['2024-02-29', '2024-03-14'], ['2024-03-07', '2024-03-21'], ['2024-03-14', '2024-03-28'], ['2024-03-21', '2024-04-04'], ['2024-03-28', '2024-04-11'], ['2024-04-04', '2024-04-18'], ['2024-04-11', '2024-04-25']] \n",
      "\n",
      "Future Accesses: [['2023-06-15', '2023-06-22'], ['2023-06-22', '2023-06-29'], ['2023-06-29', '2023-07-06'], ['2023-07-06', '2023-07-13'], ['2023-07-13', '2023-07-20'], ['2023-07-20', '2023-07-27'], ['2023-07-27', '2023-08-03'], ['2023-08-03', '2023-08-10'], ['2023-08-10', '2023-08-17'], ['2023-08-17', '2023-08-24'], ['2023-08-24', '2023-08-31'], ['2023-08-31', '2023-09-07'], ['2023-09-07', '2023-09-14'], ['2023-09-14', '2023-09-21'], ['2023-09-21', '2023-09-28'], ['2023-09-28', '2023-10-05'], ['2023-10-05', '2023-10-12'], ['2023-10-12', '2023-10-19'], ['2023-10-19', '2023-10-26'], ['2023-10-26', '2023-11-02'], ['2023-11-02', '2023-11-09'], ['2023-11-09', '2023-11-16'], ['2023-11-16', '2023-11-23'], ['2023-11-23', '2023-11-30'], ['2023-11-30', '2023-12-07'], ['2023-12-07', '2023-12-14'], ['2023-12-14', '2023-12-21'], ['2023-12-21', '2023-12-28'], ['2023-12-28', '2024-01-04'], ['2024-01-04', '2024-01-11'], ['2024-01-11', '2024-01-18'], ['2024-01-18', '2024-01-25'], ['2024-01-25', '2024-02-01'], ['2024-02-01', '2024-02-08'], ['2024-02-08', '2024-02-15'], ['2024-02-15', '2024-02-22'], ['2024-02-22', '2024-02-29'], ['2024-02-29', '2024-03-07'], ['2024-03-07', '2024-03-14'], ['2024-03-14', '2024-03-21'], ['2024-03-21', '2024-03-28'], ['2024-03-28', '2024-04-04'], ['2024-04-04', '2024-04-11'], ['2024-04-11', '2024-04-18'], ['2024-04-18', '2024-04-25'], ['2024-04-25', '2024-05-02']]\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime(2023, 6, 1)\n",
    "\n",
    "# Function to generate periods\n",
    "def generate_periods(start_date, periods, days_per_period):\n",
    "    result = []\n",
    "    \n",
    "    # Loop through the number of periods and create 15-day intervals\n",
    "    for _ in range(periods):\n",
    "        end_date = start_date + timedelta(days=days_per_period)\n",
    "        # Convert to string with the format \"%Y-%m-%d\"\n",
    "        period_str = (start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))\n",
    "        result.append(period_str)\n",
    "        start_date = end_date  # Move the start date to the end date of the current period\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "periods = generate_periods(start_date, 47, 7)\n",
    "\n",
    "p = periods[0][0]\n",
    "m = periods[1][1]\n",
    "\n",
    "train_p = []\n",
    "fut_acc_p = []\n",
    "\n",
    "for i in range(0,len(periods)-2):\n",
    "    train_p.append([periods[i][0], periods[i+1][1]])\n",
    "    fut_acc_p.append([periods[i+1][1], periods[i+2][1]])\n",
    "\n",
    "print(\"We have\", len(train_p), \"realizations \\n\")\n",
    "print(\"Train:\", train_p, '\\n')\n",
    "print(\"Future Accesses:\", fut_acc_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6966c5-4429-436d-ae6f-a43d9a8ce649",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45cb0247-4cde-4669-acd6-ab578f2f181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############  1\n",
      "Train: ['2023-06-01', '2023-06-14']\n",
      "Future Accesses: ['2023-06-15', '2023-06-22']\n",
      "\n",
      "Train: ['2023-06-01', '2023-06-14']\n",
      "Future Accesses: ['2023-06-15', '2023-06-22']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 60 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n23 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1663, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 628, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1137, in _create_dmatrix\n    return QuantileDMatrix(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 1614, in __init__\n    self._init(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 1680, in _init\n    _check_call(ret)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 310, in _check_call\n    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\nxgboost.core.XGBoostError: [23:55:01] /workspace/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\nStack trace:\n  [bt] (0) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2a6acc) [0x7f2e433bbacc]\n  [bt] (1) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5d4a71) [0x7f2e436e9a71]\n  [bt] (2) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5e86ac) [0x7f2e436fd6ac]\n  [bt] (3) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5e9449) [0x7f2e436fe449]\n  [bt] (4) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x57a041) [0x7f2e4368f041]\n  [bt] (5) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x178) [0x7f2e432cdfc8]\n  [bt] (6) /data/astro/software/alma9/conda/miniforge-24.1.2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x6a4a) [0x7f2e9678ca4a]\n  [bt] (7) /data/astro/software/alma9/conda/miniforge-24.1.2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x5fea) [0x7f2e9678bfea]\n  [bt] (8) /data/astro/software/alma9/conda/miniforge-24.1.2/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x12461) [0x7f2e967a4461]\n\n\n\n--------------------------------------------------------------------------------\n37 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1663, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 628, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1137, in _create_dmatrix\n    return QuantileDMatrix(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 1614, in __init__\n    self._init(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 1680, in _init\n    _check_call(ret)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 310, in _check_call\n    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\nxgboost.core.XGBoostError: [23:55:02] /workspace/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\nStack trace:\n  [bt] (0) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2a6acc) [0x7f2e433bbacc]\n  [bt] (1) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5d4a71) [0x7f2e436e9a71]\n  [bt] (2) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5e86ac) [0x7f2e436fd6ac]\n  [bt] (3) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5e9449) [0x7f2e436fe449]\n  [bt] (4) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x57a041) [0x7f2e4368f041]\n  [bt] (5) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x178) [0x7f2e432cdfc8]\n  [bt] (6) /data/astro/software/alma9/conda/miniforge-24.1.2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x6a4a) [0x7f2e9678ca4a]\n  [bt] (7) /data/astro/software/alma9/conda/miniforge-24.1.2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x5fea) [0x7f2e9678bfea]\n  [bt] (8) /data/astro/software/alma9/conda/miniforge-24.1.2/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x12461) [0x7f2e967a4461]\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 397\u001b[0m\n\u001b[1;32m    385\u001b[0m inner_cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    387\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m    388\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    389\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_dist,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 397\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m best_params \u001b[38;5;241m=\u001b[39m random_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[1;32m    400\u001b[0m all_best_params\u001b[38;5;241m.\u001b[39mappend(best_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1960\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1958\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1960\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:996\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    994\u001b[0m     )\n\u001b[0;32m--> 996\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[0;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 60 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n23 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1663, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 628, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1137, in _create_dmatrix\n    return QuantileDMatrix(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 1614, in __init__\n    self._init(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 1680, in _init\n    _check_call(ret)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 310, in _check_call\n    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\nxgboost.core.XGBoostError: [23:55:01] /workspace/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\nStack trace:\n  [bt] (0) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2a6acc) [0x7f2e433bbacc]\n  [bt] (1) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5d4a71) [0x7f2e436e9a71]\n  [bt] (2) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5e86ac) [0x7f2e436fd6ac]\n  [bt] (3) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5e9449) [0x7f2e436fe449]\n  [bt] (4) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x57a041) [0x7f2e4368f041]\n  [bt] (5) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x178) [0x7f2e432cdfc8]\n  [bt] (6) /data/astro/software/alma9/conda/miniforge-24.1.2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x6a4a) [0x7f2e9678ca4a]\n  [bt] (7) /data/astro/software/alma9/conda/miniforge-24.1.2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x5fea) [0x7f2e9678bfea]\n  [bt] (8) /data/astro/software/alma9/conda/miniforge-24.1.2/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x12461) [0x7f2e967a4461]\n\n\n\n--------------------------------------------------------------------------------\n37 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1663, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 628, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1137, in _create_dmatrix\n    return QuantileDMatrix(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 1614, in __init__\n    self._init(\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 1680, in _init\n    _check_call(ret)\n  File \"/nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/core.py\", line 310, in _check_call\n    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\nxgboost.core.XGBoostError: [23:55:02] /workspace/src/data/gradient_index.h:100: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`\nStack trace:\n  [bt] (0) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x2a6acc) [0x7f2e433bbacc]\n  [bt] (1) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5d4a71) [0x7f2e436e9a71]\n  [bt] (2) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5e86ac) [0x7f2e436fd6ac]\n  [bt] (3) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x5e9449) [0x7f2e436fe449]\n  [bt] (4) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x57a041) [0x7f2e4368f041]\n  [bt] (5) /nfs/pic.es/user/j/jvilalta/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGQuantileDMatrixCreateFromCallback+0x178) [0x7f2e432cdfc8]\n  [bt] (6) /data/astro/software/alma9/conda/miniforge-24.1.2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x6a4a) [0x7f2e9678ca4a]\n  [bt] (7) /data/astro/software/alma9/conda/miniforge-24.1.2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x5fea) [0x7f2e9678bfea]\n  [bt] (8) /data/astro/software/alma9/conda/miniforge-24.1.2/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x12461) [0x7f2e967a4461]\n\n\n"
     ]
    }
   ],
   "source": [
    "retrain = False\n",
    "useFiles = False\n",
    "createModel = True\n",
    "\n",
    "rl_a = []\n",
    "\n",
    "accesses_train = []\n",
    "accesses_fut_acc = []\n",
    "\n",
    "unique_files_train = []\n",
    "unique_files_fut_acc = []\n",
    "\n",
    "all_preds_by_o = {}\n",
    "all_labels_by_o = {}\n",
    "\n",
    "roc_curves = []\n",
    "pr_curves = []\n",
    "\n",
    "importances_by_period = {}\n",
    "\n",
    "\n",
    "# directory to save XGBoost decision trees\n",
    "path1 = f\"./XGBoost/raw/trees/Decision_Tree_XGB\"\n",
    "\n",
    "# directory to save XGBoost results\n",
    "path2 = f\"./XGBoost/raw/results/Results_XGB\"\n",
    "\n",
    "\n",
    "gif_dir = os.path.join(path2, \"gif\")\n",
    "results_txt = os.path.join(path2, \"Results.txt\")\n",
    "\n",
    "os.makedirs(path1, exist_ok=True)\n",
    "os.makedirs(gif_dir, exist_ok=True)\n",
    "\n",
    "if os.path.exists(results_txt):\n",
    "    os.remove(results_txt)\n",
    "    \n",
    "all_preds_by_o[o] = []\n",
    "all_labels_by_o[o] = []\n",
    "\n",
    "results = {\n",
    "    \"auc\": [[], []],\n",
    "    \"acc\": [[], []],\n",
    "    \"rec\": [[], []],\n",
    "    \"prec\": [[], []],\n",
    "    \"f1\": [[], []]\n",
    "}\n",
    "\n",
    "f = open(f\"{path2}/Results.txt\", \"a\")\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for rl in range(0,len(train_p)):\n",
    "    ii+=1\n",
    "    print(\"############ \", ii)\n",
    "    rl_a.append(str(rl+1))\n",
    "\n",
    "    # Sample generation\n",
    "\n",
    "    df1 = df_orig[ (df_orig['date'] >= train_p[rl][0]) & (df_orig['date'] < train_p[rl][1])].copy()\n",
    "    df2 = df_orig[ (df_orig['date'] >= fut_acc_p[rl][0]) & (df_orig['date'] < fut_acc_p[rl][1])].copy()\n",
    "\n",
    "    accesses_train.append(df1.shape[0])\n",
    "    accesses_fut_acc.append(df2.shape[0])\n",
    "    unique_files_train.append(len(df1['filename'].unique()))\n",
    "    unique_files_fut_acc.append(len(df2['filename'].unique()))\n",
    "   \n",
    "    # Add new column for the Accesses in the train    \n",
    "    df1['Access'] = df1.groupby(['filename']).cumcount().add(1)\n",
    "    df1['Total Accesses'] = df1.groupby('filename').filename.transform('count')\n",
    "    \n",
    "    # Add new column with the read access to period\n",
    "    date_string = df2['date'].max() + \" 23:59:59.999999\"\n",
    "    dt_object = datetime.strptime(date_string, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    epoch_time_period1 = dt_object.timestamp()\n",
    "    df1['read_access time'] = (epoch_time_period1 - df1['time'])\n",
    "\n",
    "    # Add new column for the Future Access in the train and test periods       \n",
    "    df1['future_acc'] = df1.filename.isin(df2.filename).astype(int)  \n",
    "    df1.sort_values(['filename','time'], ascending=[True,True], inplace = True)\n",
    "            \n",
    "    # Calculate the last 5 accesses\n",
    "    df1[\"5th last read access\"] = df1['read_access time'].shift(4)\n",
    "    df1[\"4th last read access\"] = df1['read_access time'].shift(3)\n",
    "    df1[\"3rd last read access\"] = df1['read_access time'].shift(2)\n",
    "    df1[\"2nd last read access\"] = df1['read_access time'].shift(1)\n",
    "    df1[\"last read access\"] = df1['read_access time']\n",
    "\n",
    "    df1.loc[df1['Total Accesses'] == 1, '2nd last read access'] = np.nan\n",
    "    df1.loc[df1['Total Accesses'] == 1, '3rd last read access'] = np.nan\n",
    "    df1.loc[df1['Total Accesses'] == 1, '4th last read access'] = np.nan\n",
    "    df1.loc[df1['Total Accesses'] == 1, '5th last read access'] = np.nan\n",
    "        \n",
    "    df1.loc[df1['Total Accesses'] == 2, '3rd last read access'] = np.nan\n",
    "    df1.loc[df1['Total Accesses'] == 2, '4th last read access'] = np.nan\n",
    "    df1.loc[df1['Total Accesses'] == 2, '5th last read access'] = np.nan\n",
    "        \n",
    "    df1.loc[df1['Total Accesses'] == 3, '4th last read access'] = np.nan\n",
    "    df1.loc[df1['Total Accesses'] == 3, '5th last read access'] = np.nan\n",
    "        \n",
    "    df1.loc[df1['Total Accesses'] == 4, '5th last read access'] = np.nan\n",
    "        \n",
    "    # Calculate first accesses\n",
    "    df1.sort_values(['filename','time'], ascending=[True,True], inplace = True)\n",
    "\n",
    "    df1_first=df1.drop_duplicates(subset = ['filename'], keep = 'first', inplace = False)\n",
    "    df1_last=df1.drop_duplicates(subset = ['filename'], keep = 'last', inplace = False)\n",
    "    df1_first_list=list(df1_first[\"time\"])\n",
    "    df1_last_list=list(df1_last[\"time\"])\n",
    "        \n",
    "    # DeltaT 1st access and most recent:\n",
    "    dt_1st_to_last = [(x1 - x2) for (x1, x2) in zip(df1_last_list, df1_first_list)]\n",
    "    dt_1st_to_last = [0 if x==0 else x for x in dt_1st_to_last]\n",
    "        \n",
    "    # Recency 1st access\n",
    "    recency_1st = [(epoch_time_period1 - x) for x in df1_first_list]\n",
    "        \n",
    "    df1.sort_values(['filename','time'], ascending=[True,True], inplace = True)\n",
    "    df1_final = df1.copy()\n",
    "    df1_final.drop_duplicates(subset = ['filename'], keep = 'last', inplace = True)\n",
    "    df1_final.sort_values(by='filename', ascending=True, inplace = True)\n",
    "        \n",
    "    df1_final.loc[:, 'deltaT_1_last'] = dt_1st_to_last\n",
    "    df1_final.loc[:, 'recency_1st'] = recency_1st\n",
    "    \n",
    "    df1_final['future_acc'] = df1_final.pop('future_acc')\n",
    "\n",
    "    \n",
    "    if ii <=1:\n",
    "        df1_final_cp = df1_final.copy()\n",
    "    else:\n",
    "        df1_final_cp = pd.concat([df1_final.copy(), df1_final_cp])\n",
    "\n",
    "    \n",
    "    threshold_time = pd.to_datetime(df1_final_cp['date'].max()) - pd.DateOffset(months=3)\n",
    "    df1_final_cp = df1_final_cp[df1_final_cp['date'] >= threshold_time.strftime(\"%Y-%m-%d\")]\n",
    "    df1_final_cp = df1_final_cp[df1_final_cp['date'] >= threshold_time.strftime(\"%Y-%m-%d\")]\n",
    "    \n",
    "\n",
    "    df1_final_c = df1_final_cp.copy()\n",
    "    \n",
    "    f_test=0.3\n",
    "    \n",
    "    df1_final_c.sort_values(by='time', ascending=True, inplace=True)\n",
    "    \n",
    "\n",
    "    ##########################\n",
    "    # Undersampling\n",
    "    # df1_final_var = df1_final_c.drop(columns=['future_acc']).copy()\n",
    "    # df1_final_label = df1_final_c['future_acc']\n",
    "\n",
    "    # under = RandomUnderSampler(sampling_strategy=1)\n",
    "    # df1_final_var_under, df1_final_label_under = under.fit_resample(df1_final_var, df1_final_label)\n",
    "\n",
    "    # df1_final_under = pd.concat([df1_final_var_under, df1_final_label_under], axis=1)\n",
    "\n",
    "    # string_train_date = f\"Train: ['{df1_final_under['date'].min()}', '{df1_final_under['date'].max()}']\"\n",
    "    # string_fut_date = f\"Future Accesses: {str(fut_acc_p[rl])}\\n\"\n",
    "\n",
    "    # print(f\"Train: ['{df1_final_under['date'].min()}', '{df1_final_under['date'].max()}']\")\n",
    "    # print(f\"Future Accesses: {str(fut_acc_p[rl])}\\n\")\n",
    "\n",
    "    # df1_final = df1_final_under.copy()\n",
    "\n",
    "    \n",
    "    ##########################\n",
    "    # Tomek links\n",
    "\n",
    "    # df1_final_label = df1_final_c['future_acc']\n",
    "\n",
    "    # df1_final_var = df1_final_c.select_dtypes(include=['number'])\n",
    "    # df1_final_var = df1_final_var.drop(columns=['future_acc'])\n",
    "    \n",
    "    # df1_final_var = df1_final_var.fillna(-1.0)\n",
    "    \n",
    "    # tl = TomekLinks(sampling_strategy='auto')\n",
    "    # df1_final_var_under, df1_final_label_under = tl.fit_resample(df1_final_var, df1_final_label)\n",
    "\n",
    "    # df1_final_under = pd.concat([df1_final_var_under, df1_final_label_under], axis=1)\n",
    "\n",
    "    # df1_final_under = df1_final_under.replace(-1.0, np.nan)\n",
    "\n",
    "    # df1_final = df1_final_under.copy()\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    # Edited Nearest Neighbours\n",
    "\n",
    "    # df1_final_label = df1_final_c['future_acc']\n",
    "\n",
    "    # df1_final_var = df1_final_c.select_dtypes(include=['number'])\n",
    "    # df1_final_var = df1_final_var.drop(columns=['future_acc'])\n",
    "    \n",
    "    # df1_final_var = df1_final_var.fillna(-1.0)\n",
    "    \n",
    "    # enn = EditedNearestNeighbours(sampling_strategy='auto')\n",
    "    # df1_final_var_under, df1_final_label_under = enn.fit_resample(df1_final_var, df1_final_label)\n",
    "\n",
    "    # df1_final_under = pd.concat([df1_final_var_under, df1_final_label_under], axis=1)\n",
    "\n",
    "    # df1_final_under = df1_final_under.replace(-1.0, np.nan)\n",
    "\n",
    "    # df1_final = df1_final_under.copy()\n",
    "\n",
    "    ##########################\n",
    "    #SMOTE\n",
    "    # string_train_date = f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\"\n",
    "    # string_fut_date = f\"Future Accesses: {str(fut_acc_p[rl])}\\n\"\n",
    "\n",
    "    # print(f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\")\n",
    "    # print(f\"Future Accesses: {str(fut_acc_p[rl])}\\n\")\n",
    "\n",
    "    # df1_final_c = df1_final_c.sample(frac = 1).copy()\n",
    "\n",
    "    # df1_final_c = df1_final_c.fillna(-1.0)\n",
    "    \n",
    "    # df1_final_var = df1_final_c.drop(columns=['future_acc']).copy()\n",
    "    # df1_final_label = df1_final_c['future_acc']\n",
    "    \n",
    "    # df1_final_var_numeric = df1_final_var.select_dtypes(include=['number'])\n",
    "\n",
    "    # smote = SMOTE(sampling_strategy=1)\n",
    "    # df1_final_var_resampled, df1_final_label_resampled = smote.fit_resample(df1_final_var_numeric, df1_final_label)\n",
    "\n",
    "    # df1_final_resampled = pd.concat([df1_final_var_resampled, df1_final_label_resampled], axis=1)\n",
    "\n",
    "    # df1_final_resampled['future_acc'] = df1_final_resampled['future_acc'].astype(int)\n",
    "\n",
    "    # df1_final_resampled = df1_final_resampled.replace(-1.0, np.nan)\n",
    "    \n",
    "    # df1_final = df1_final_resampled.copy()\n",
    "\n",
    "    ##########################\n",
    "    #SMOTEEN\n",
    "    # string_train_date = f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\"\n",
    "    # string_fut_date = f\"Future Accesses: {str(fut_acc_p[rl])}\\n\"\n",
    "\n",
    "    # print(f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\")\n",
    "    # print(f\"Future Accesses: {str(fut_acc_p[rl])}\\n\")\n",
    "\n",
    "    # df1_final = df1_final_c.sample(frac = 1).copy()\n",
    "\n",
    "    # df1_final_c = df1_final_c.fillna(-1.0)\n",
    "    \n",
    "    # df1_final_var = df1_final_c.drop(columns=['future_acc']).copy()\n",
    "    # df1_final_label = df1_final_c['future_acc']\n",
    "    \n",
    "    # df1_final_var_numeric = df1_final_var.select_dtypes(include=['number'])\n",
    "\n",
    "    # smoteenn = SMOTEENN(sampling_strategy=1)\n",
    "    # df1_final_var_resampled, df1_final_label_resampled = smoteenn.fit_resample(df1_final_var_numeric, df1_final_label)\n",
    "\n",
    "    # df1_final_resampled = pd.concat([df1_final_var_resampled, df1_final_label_resampled], axis=1)\n",
    "\n",
    "    # df1_final_resampled['future_acc'] = df1_final_resampled['future_acc'].astype(int)\n",
    "\n",
    "    # df1_final_resampled = df1_final_resampled.replace(-1.0, np.nan)\n",
    "\n",
    "    # df1_final = df1_final_resampled.copy()\n",
    "\n",
    "    ##########################\n",
    "    #SMOTE TOMEK\n",
    "    # string_train_date = f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\"\n",
    "    # string_fut_date = f\"Future Accesses: {str(fut_acc_p[rl])}\\n\"\n",
    "\n",
    "    # print(f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\")\n",
    "    # print(f\"Future Accesses: {str(fut_acc_p[rl])}\\n\")\n",
    "\n",
    "    # df1_final = df1_final_c.sample(frac = 1).copy()\n",
    "\n",
    "    # df1_final_c = df1_final_c.fillna(-1.0)\n",
    "    \n",
    "    # df1_final_var = df1_final_c.drop(columns=['future_acc']).copy()\n",
    "    # df1_final_label = df1_final_c['future_acc']\n",
    "    \n",
    "    # df1_final_var_numeric = df1_final_var.select_dtypes(include=['number'])\n",
    "\n",
    "    # smtk = SMOTETomek(sampling_strategy=1)\n",
    "    # df1_final_var_resampled, df1_final_label_resampled = smtk.fit_resample(df1_final_var_numeric, df1_final_label)\n",
    "\n",
    "    # df1_final_resampled = pd.concat([df1_final_var_resampled, df1_final_label_resampled], axis=1)\n",
    "\n",
    "    # df1_final_resampled['future_acc'] = df1_final_resampled['future_acc'].astype(int)\n",
    "\n",
    "    # df1_final_resampled = df1_final_resampled.replace(-1.0, np.nan)\n",
    "\n",
    "    # df1_final = df1_final_resampled.copy()\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    # No class balance\n",
    "\n",
    "    string_train_date = f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\"\n",
    "    string_fut_date = f\"Future Accesses: {str(fut_acc_p[rl])}\\n\"\n",
    "\n",
    "    print(f\"Train: ['{df1_final_c['date'].min()}', '{df1_final_c['date'].max()}']\")\n",
    "    print(f\"Future Accesses: {str(fut_acc_p[rl])}\\n\")\n",
    "\n",
    "    df1_final = df1_final_c.sample(frac = 1).copy()\n",
    "\n",
    "    \n",
    "    ##########################\n",
    "    # End of class balance methods\n",
    "\n",
    "\n",
    "\n",
    "    # -----Continuation-----\n",
    "    # Apply a logarithmic transformation to the required columns\n",
    "\n",
    "    log_vars =  ['Total Accesses', '5th last read access', '4th last read access',\n",
    "                 '3rd last read access', '2nd last read access','last read access',\n",
    "                'recency_1st', 'deltaT_1_last']\n",
    "\n",
    "    for var in log_vars:\n",
    "        df1_final[var] = np.log1p(df1_final[var])\n",
    "        \n",
    "    \n",
    "\n",
    "    thevars = ['last read access','recency_1st', 'deltaT_1_last', '2nd last read access',\n",
    "               '3rd last read access', '4th last read access','5th last read access', \n",
    "               'Total Accesses', 'd_label_encoded', 'future_acc']\n",
    "\n",
    "    \n",
    "    df1_final['future_acc'] = df1_final['future_acc'].astype(int)\n",
    "    \n",
    "    df1_final = df1_final[thevars]\n",
    "\n",
    "\n",
    "    # Nested Cross Validation\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    all_best_params = []\n",
    "\n",
    "    auc_scores, acc_scores, rec_scores, prec_scores, f1_scores = [], [], [], [], []\n",
    "    \n",
    "    for outer_fold, (train_val_idx, test_idx) in enumerate(outer_cv.split(df1_final.drop(columns=['future_acc']), df1_final['future_acc'])):\n",
    "    \n",
    "        df1_train_val = df1_final.iloc[train_val_idx]\n",
    "        df1_test = df1_final.iloc[test_idx]\n",
    "\n",
    "        train_valL = df1_train_val['future_acc'].values\n",
    "        testL = df1_test['future_acc'].values\n",
    "    \n",
    "        train_valD = df1_train_val.drop(columns=['future_acc']).values\n",
    "        testD = df1_test.drop(columns=['future_acc']).values\n",
    "    \n",
    "\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=f_test)\n",
    "        for train_index, val_index in sss.split(train_valD, train_valL):\n",
    "            trainD, valD = train_valD[train_index], train_valD[val_index]\n",
    "            trainL, valL = train_valL[train_index], train_valL[val_index]\n",
    "    \n",
    "        # Calculate scale_pos_weight to handle class imbalance within the fold\n",
    "        scale = len(trainL[trainL == 0]) / len(trainL[trainL == 1])\n",
    "\n",
    "        \n",
    "        param_dist = {\n",
    "            'n_estimators': randint(100, 400),\n",
    "            'max_depth': randint(3, 10),\n",
    "            'learning_rate': uniform(0.01, 0.3),\n",
    "            'subsample': uniform(0.7, 0.3),\n",
    "            'colsample_bytree': uniform(0.7, 0.3),\n",
    "            'reg_lambda': uniform(0, 2),\n",
    "            'reg_alpha': uniform(0, 1)\n",
    "        }\n",
    "    \n",
    "        model = XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            tree_method='gpu_hist',\n",
    "            predictor='gpu_predictor',\n",
    "            # scale_pos_weight=scale\n",
    "        )\n",
    "\n",
    "        inner_cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    \n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=20,\n",
    "            cv=inner_cv,\n",
    "            n_jobs=1,\n",
    "            scoring='roc_auc',\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "        random_search.fit(trainD, trainL)\n",
    "    \n",
    "        best_params = random_search.best_params_\n",
    "        all_best_params.append(best_params)\n",
    "    \n",
    "        dtrain = xgb.DMatrix(trainD, label=trainL)\n",
    "        dval = xgb.DMatrix(valD, label=valL)\n",
    "    \n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss',\n",
    "            'tree_method': 'hist',\n",
    "            'device': 'cuda',\n",
    "            # 'scale_pos_weight': scale,\n",
    "            **best_params\n",
    "        }\n",
    "    \n",
    "        model_fold = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dval, 'val')],\n",
    "            early_stopping_rounds=10,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "    \n",
    "        dtest = xgb.DMatrix(testD)\n",
    "        pred_proba = model_fold.predict(dtest)\n",
    "\n",
    "        pred_labels = (pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        auc_score = roc_auc_score(testL, pred_proba)\n",
    "        acc = accuracy_score(testL, pred_labels)\n",
    "        rec = recall_score(testL, pred_labels)\n",
    "        prec = precision_score(testL, pred_labels)\n",
    "        f1 = f1_score(testL, pred_labels)\n",
    "    \n",
    "        # print(f\"AUC Fold {outer_fold+1}: {auc_score:.4f}\")\n",
    "        \n",
    "        auc_scores.append(auc_score)\n",
    "        acc_scores.append(acc)\n",
    "        rec_scores.append(rec)\n",
    "        prec_scores.append(prec)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "    \n",
    "    results[\"auc\"][0].append(np.mean(auc_scores))\n",
    "    results[\"auc\"][1].append(np.std(auc_scores, ddof = 1)/np.sqrt(len(auc_scores)))\n",
    "    \n",
    "    results[\"acc\"][0].append(np.mean(acc_scores))\n",
    "    results[\"acc\"][1].append(np.std(acc_scores, ddof = 1)/np.sqrt(len(acc_scores)))\n",
    "    \n",
    "    results[\"rec\"][0].append(np.mean(rec_scores))\n",
    "    results[\"rec\"][1].append(np.std(rec_scores, ddof = 1)/np.sqrt(len(rec_scores)))\n",
    "    \n",
    "    results[\"prec\"][0].append(np.mean(prec_scores))\n",
    "    results[\"prec\"][1].append(np.std(prec_scores, ddof = 1)/np.sqrt(len(prec_scores)))\n",
    "    \n",
    "    results[\"f1\"][0].append(np.mean(f1_scores))\n",
    "    results[\"f1\"][1].append(np.std(f1_scores, ddof = 1)/np.sqrt(len(f1_scores)))\n",
    "    \n",
    "    # print(f\"Mean AUC (Nested CV): {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "\n",
    "    param_sums = defaultdict(float)\n",
    "    \n",
    "    for bp in all_best_params:\n",
    "        for key, value in bp.items():\n",
    "            param_sums[key] += value\n",
    "    \n",
    "    int_params = ['n_estimators', 'max_depth']\n",
    "\n",
    "    best_params_avg = {}\n",
    "    for key in param_sums:\n",
    "        mean_value = param_sums[key] / len(all_best_params)\n",
    "        if key in int_params:\n",
    "            best_params_avg[key] = int(round(mean_value))\n",
    "        else:\n",
    "            best_params_avg[key] = float(mean_value)\n",
    "    \n",
    "    # print(\"Best_params average:\", best_params_avg)\n",
    "\n",
    "    dtrain_final = xgb.DMatrix(df1_final.drop(columns=['future_acc']).values, label=df1_final['future_acc'].values)\n",
    "    \n",
    "    scale = len(trainL[trainL == 0]) / len(trainL[trainL == 1])\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda',\n",
    "        # 'scale_pos_weight': scale,\n",
    "        **best_params_avg\n",
    "    }\n",
    "    \n",
    "\n",
    "    model_final = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain_final,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain_final, 'train')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    model_final.save_model(f'{path1}/model_xgb_{int(epoch_time_period1)}.ubj')\n",
    "    \n",
    "    dtest = xgb.DMatrix(testD)\n",
    "    pred_proba = model_final.predict(dtest)  \n",
    "    pred_labels = (pred_proba > 0.5).astype(int)\n",
    "\n",
    "    \n",
    "    ###############\n",
    "    #Predicted probability distribution by period\n",
    "    pred_probs_reaccessed = pred_proba[testL  == 1]  \n",
    "    pred_probs_non_reaccessed = pred_proba[testL  == 0] \n",
    "    \n",
    "    plt.hist(pred_probs_reaccessed, bins=50, color='blue', alpha=0.7, label='Reaccessed', histtype='step', linewidth=2)\n",
    "    plt.hist(pred_probs_non_reaccessed, bins=50, color='orange', alpha=0.7, label='Not Reaccessed', histtype='step', linewidth=2)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Probabilities')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.ylim(1, 100000)\n",
    "    plt.text(0.88, 2300, ii , fontsize=14, ha='left', va='bottom')\n",
    "    plt.savefig(f\"{path2}/gif/hist_gif{ii}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    all_preds_by_o[o].append(pred_proba)\n",
    "    all_labels_by_o[o].append(testL)\n",
    "\n",
    "    accuracy = accuracy_score(testL, pred_labels)\n",
    "    recall = recall_score(testL, pred_labels)\n",
    "    precision = precision_score(testL, pred_labels)\n",
    "    f1 = f1_score(testL, pred_labels)\n",
    "    auc_a = roc_auc_score(testL, pred_proba)\n",
    "\n",
    "    print(f\"\\n--- Results ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc_a:.4f}\\n\")\n",
    "\n",
    "    importances_by_period[o] = []\n",
    "\n",
    "    feature_names = df1_final.drop(columns=['future_acc']).columns.tolist()\n",
    "\n",
    "    importance_dict = model_final.get_score(importance_type='weight')\n",
    "\n",
    "    mapped_importance = {\n",
    "        feature_names[int(k[1:])]: v for k, v in importance_dict.items()\n",
    "        if k.startswith('f') and k[1:].isdigit() and int(k[1:]) < len(feature_names)\n",
    "    }\n",
    "\n",
    "    importances_by_period[o].append(mapped_importance)\n",
    "\n",
    "    \n",
    "    \n",
    "###############\n",
    "#Output results\n",
    "str_to_file = (\n",
    "    f\"{','.join(map(str, results['auc'][0]))}\\n\"\n",
    "    f\"{','.join(map(str, results['auc'][1]))}\\n\"\n",
    "    f\"{','.join(map(str, results['acc'][0]))}\\n\"\n",
    "    f\"{','.join(map(str, results['acc'][1]))}\\n\"\n",
    "    f\"{','.join(map(str, results['rec'][0]))}\\n\"\n",
    "    f\"{','.join(map(str, results['rec'][1]))}\\n\"\n",
    "    f\"{','.join(map(str, results['prec'][0]))}\\n\"\n",
    "    f\"{','.join(map(str, results['prec'][1]))}\\n\"\n",
    "    f\"{','.join(map(str, results['f1'][0]))}\\n\"\n",
    "    f\"{','.join(map(str, results['f1'][1]))}\\n\"\n",
    ")\n",
    "\n",
    "with open(f\"{path2}/Results.txt\", \"a\") as f:\n",
    "    f.write(str_to_file)\n",
    "        \n",
    "\n",
    "all_preds_flat = np.concatenate(all_preds_by_o[o])\n",
    "all_labels_flat = np.concatenate(all_labels_by_o[o])\n",
    "\n",
    "overall_auc = roc_auc_score(all_labels_flat, all_preds_flat)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(all_labels_flat, all_preds_flat)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "roc_curves.append((fpr, tpr, roc_auc, o))\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(all_labels_flat, all_preds_flat)\n",
    "ap = average_precision_score(all_labels_flat, all_preds_flat)\n",
    "pr_curves.append((recall, precision, ap, o))\n",
    "\n",
    "\n",
    "###############\n",
    "#Total redicted probability distribution\n",
    "pred_probs_reaccessed = all_preds_flat[all_labels_flat  == 1]  \n",
    "pred_probs_non_reaccessed = all_preds_flat[all_labels_flat  == 0] \n",
    "\n",
    "plt.hist(pred_probs_reaccessed, bins=50, color='blue', alpha=0.7, label='Reaccessed', histtype='step', linewidth=2)\n",
    "plt.hist(pred_probs_non_reaccessed, bins=50, color='orange', alpha=0.7, label='Not reaccessed', histtype='step', linewidth=2)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Probabilities')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path2}/hist_probabilities.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "###############\n",
    "#Confusion matrix\n",
    "all_pred_labels = (all_preds_flat > 0.5).astype(int)\n",
    "\n",
    "class_names = ['Not reaccessed', 'Reaccessed']\n",
    "\n",
    "cm = confusion_matrix(all_labels_flat, all_pred_labels)\n",
    "\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=0, keepdims=True) * 100\n",
    "\n",
    "plt.imshow(cm_percent, interpolation='nearest', cmap='Blues')\n",
    "plt.colorbar(label='% of prediction')\n",
    "\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "labels = np.array([['TN', 'FP'], ['FN', 'TP']])\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        pct = cm_percent[i, j]\n",
    "        plt.text(j, i, f\"{labels[i, j]} = {pct:.1f}%\", \n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if pct > cm_percent.max()/2 else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Prediction label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path2}/confusion_matrix.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###############\n",
    "#Feature Importance\n",
    "average_importance = defaultdict(float)\n",
    "count_importance = defaultdict(int)\n",
    "\n",
    "for imp in importances_by_period[o]:\n",
    "    for feat, val in imp.items():\n",
    "        average_importance[feat] += val\n",
    "        count_importance[feat] += 1\n",
    "\n",
    "for feat in average_importance:\n",
    "    average_importance[feat] /= count_importance[feat]\n",
    "\n",
    "importance_df = pd.DataFrame(\n",
    "    list(average_importance.items()), columns=['Feature', 'Importance']\n",
    ").sort_values(by='Importance', ascending=True)\n",
    "\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Average Importance')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path2}/feature_importance.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###############\n",
    "#Correlation matrix\n",
    "feature_columns = df1_final.drop(columns=['future_acc']).columns\n",
    "\n",
    "corr_matrix = df1_final[feature_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='viridis', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path2}/correlation_matrix.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###############\n",
    "#Precision-Recall\n",
    "for recall, precision, ap, o in pr_curves:\n",
    "    plt.plot(recall, precision, lw=1.5, label=f'AP = {ap:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid()\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path2}/corves_precision_recall.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "with open(f\"{path2}/pr_curves_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pr_curves, f)\n",
    "\n",
    "###############\n",
    "#ROC Curves\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey', lw=1)\n",
    "for fpr, tpr, roc_auc, o in roc_curves:\n",
    "    plt.plot(fpr, tpr, lw=1.5, label=f'AUC = {roc_auc:.4f}')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path2}/corves_roc.png\", dpi=300)   \n",
    "plt.show()\n",
    "\n",
    "with open(f\"{path2}/roc_curves_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(roc_curves, f)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jvilalta",
   "language": "python",
   "name": "jvilalta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
